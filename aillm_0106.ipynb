{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0tR+2CDbzBILKNcU4tgv4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HUJameson/Colab/blob/main/aillm_0106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "TPNgtkRQzqRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Xxp67aHOEp",
        "outputId": "95a415ac-c37f-4ca5-ab75-6b7b4a33adeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from sk_utils import read_sk\n",
        "openai_sk = read_sk()\n",
        "%env OPENAI_API_KEY=$openai_sk"
      ],
      "metadata": {
        "id": "AiswPak1xbrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Wa-fP5Nxxpd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvmGYUy2y9jN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Conversation:\n",
        "    def __init__(self, prompt, num_of_round):\n",
        "        self.prompt = prompt\n",
        "        self.num_of_round = num_of_round\n",
        "        self.messages = []\n",
        "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
        "\n",
        "    def ask(self, question):\n",
        "        try:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": question})\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.5,\n",
        "                max_tokens=2048,\n",
        "                top_p=1,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return e\n",
        "\n",
        "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
        "\n",
        "        if len(self.messages) > self.num_of_round*2 + 1:\n",
        "            del self.messages[1:3]\n",
        "        return message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n",
        "1. 你的回答必须是中文\n",
        "2. 回答限制在100个字以内\"\"\"\n",
        "conv1 = Conversation(prompt, 2)\n",
        "question1 = \"你是谁？\"\n",
        "print(\"User : %s\" % question1)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question1))\n",
        "\n",
        "question2 = \"请问鱼香肉丝怎么做？\"\n",
        "print(\"User : %s\" % question2)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question2))\n",
        "\n",
        "question3 = \"那蚝油牛肉呢？\"\n",
        "print(\"User : %s\" % question3)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yHPCQcjzSEN",
        "outputId": "ad88ac3e-7f32-4e43-a854-5802de19c4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : 你是谁？\n",
            "Assistant : 我是一个中国厨师，专门负责烹饪美食。有什么菜品需要我帮助吗？\n",
            "\n",
            "User : 请问鱼香肉丝怎么做？\n",
            "Assistant : 鱼香肉丝是一道经典的川菜，制作方法如下：\n",
            "1. 将猪肉切成细丝，加入少许盐、料酒、淀粉拌匀腌制10分钟。\n",
            "2. 锅中加油烧热，放入蒜末、姜末煸炒出香味。\n",
            "3. 加入腌制好的猪肉丝翻炒至变色。\n",
            "4. 加入青椒丝、红椒丝、木耳丝翻炒均匀。\n",
            "5. 加入酱油、糖、醋、料酒、鸡精、水淀粉调成的鱼香汁，翻炒均匀收汁即可。\n",
            "希望你喜欢这道鱼香肉丝！\n",
            "\n",
            "User : 那蚝油牛肉呢？\n",
            "Assistant : 蚝油牛肉是一道香气扑鼻的菜品，制作方法如下：\n",
            "1. 将牛肉切成薄片，加入盐、生粉、料酒腌制10分钟。\n",
            "2. 锅中加油烧热，放入蒜末、姜末煸炒出香味。\n",
            "3. 加入腌制好的牛肉片翻炒至变色。\n",
            "4. 加入蚝油、酱油、糖、生抽、水淀粉调成的调味汁，翻炒均匀。\n",
            "5. 最后加入青椒丝、红椒丝翻炒均匀即可。\n",
            "希望你喜欢这道蚝油牛肉！\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question4 = \"我问你的第一个问题是什么？\"\n",
        "print(\"User : %s\" % question4)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqRyg0L00C3u",
        "outputId": "4aa05310-3caf-4be4-a17e-af525b3a0c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : 我问你的第一个问题是什么？\n",
            "Assistant : 你的第一个问题是关于鱼香肉丝的制作方法。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question5 = \"我问你的第一个问题是什么？\"\n",
        "print(\"User : %s\" % question5)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1TjdU-40O7F",
        "outputId": "f146ad80-0585-44a2-a1be-df4132dfccb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : 我问你的第一个问题是什么？\n",
            "Assistant : 非常抱歉，我误解了你的问题。你的第一个问题是关于蚝油牛肉的制作方法。对不起给你带来困扰。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question6 = \"我问你的第一个问题是什么？\"\n",
        "print(\"User : %s\" % question6)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biNbp_6x0ZrU",
        "outputId": "bad89003-c4ae-4ec8-d36e-ab9321028475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : 我问你的第一个问题是什么？\n",
            "Assistant : 非常抱歉，我之前的回答有误。请问您的第一个问题是什么？我会尽力回答。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conversation2:\n",
        "    def __init__(self, prompt, num_of_round):\n",
        "        self.prompt = prompt\n",
        "        self.num_of_round = num_of_round\n",
        "        self.messages = []\n",
        "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
        "\n",
        "    def ask(self, question):\n",
        "        try:\n",
        "            self.messages.append( {\"role\": \"user\", \"content\": question})\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.5,\n",
        "                max_tokens=2048,\n",
        "                top_p=1,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return e\n",
        "\n",
        "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        num_of_tokens = response['usage']['total_tokens']\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
        "\n",
        "        if len(self.messages) > self.num_of_round*2 + 1:\n",
        "            del self.messages[1:3]\n",
        "        return message, num_of_tokens"
      ],
      "metadata": {
        "id": "39TO6vdV1zmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv2 = Conversation2(prompt, 3)\n",
        "questions = [question1, question2, question3, question4, question5]\n",
        "for question in questions:\n",
        "    answer, num_of_tokens = conv2.ask(question)\n",
        "    print(\"询问 {%s} 消耗的token数量是 : %d\" % (question, num_of_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "93hol9Q-16MN",
        "outputId": "99b48810-1d14-44ff-b4b6-128c73d99962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "询问 {你是谁？} 消耗的token数量是 : 122\n",
            "询问 {请问鱼香肉丝怎么做？} 消耗的token数量是 : 384\n",
            "询问 {那蚝油牛肉呢？} 消耗的token数量是 : 621\n",
            "Rate limit reached for default-gpt-3.5-turbo in organization org-LsRSFcCdezFMWrF1yYEd6eYe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e38658374a15>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"询问 {%s} 消耗的token数量是 : %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable RateLimitError object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "Gmve3E7c3GGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "conv2 = Conversation2(prompt, 3)\n",
        "question1 = \"你是谁？\"\n",
        "answer1, num_of_tokens = conv2.ask(question1)\n",
        "print(\"总共消耗的token数量是 : %d\" % (num_of_tokens))\n",
        "\n",
        "prompt_count = len(encoding.encode(prompt))\n",
        "question1_count = len(encoding.encode(question1))\n",
        "answer1_count = len(encoding.encode(answer1))\n",
        "total_count = prompt_count + question1_count + answer1_count\n",
        "print(\"Prompt消耗 %d Token, 问题消耗 %d Token，回答消耗 %d Token，总共消耗 %d Token\" % (prompt_count, question1_count, answer1_count, total_count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FacG04Fe29ad",
        "outputId": "a71e54a3-2740-4a6d-cd93-462f1d553400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "总共消耗的token数量是 : 129\n",
            "Prompt消耗 65 Token, 问题消耗 5 Token，回答消耗 48 Token，总共消耗 118 Token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "rnIn7J2D55b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "prompt = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n",
        "1. 你的回答必须是中文\n",
        "2. 回答限制在100个字以内\"\"\"\n",
        "\n",
        "conv = Conversation(prompt, 10)\n",
        "\n",
        "def answer(question, history=[]):\n",
        "    history.append(question)\n",
        "    response = conv.ask(question)\n",
        "    history.append(response)\n",
        "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "    return responses, history\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot{height:300px} .overflow-y-auto{height:500px}\") as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
        "\n",
        "    txt.submit(answer, [txt, state], [chatbot, state])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "lPakESy_51f6",
        "outputId": "86842690-1e52-4d33-84ee-850ff5de01ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-5eb9a27b56bb>:22: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://d57423a62c754ab09c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d57423a62c754ab09c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}